\chapter{預測使用者個人資訊與大六性格特質分數之方法}
{
首先定義預測使用者個人資訊與大六性格特質分數分別為分類問題以及回歸問題。對於使用者個人資訊之預測，先將 3 種個人資訊進行分類（1）性別分為：男性、女性以及其他，共 3 類、（2）年齡分為：$16\sim20$、$21\sim25$、$26\sim30$、$31\sim35$、$36\sim40$、$41\sim45$ 以及 $46$ 以上，共 7 類、（3）感情狀態分為：單身、交往中、已婚、其他以及不公開，共 5 類。而大六性格特質分數則是介於 0 至 50 分，分別為真誠性（{\se Honesty-Humility}）、情緒不穩定性（{\se Neuroticism}）、外向性（{\se Extraversion}）、親和性（{\se Agreeableness}）、盡責性（{\se Conscientiousness}）以及經驗開放性（{\se Openness to Experience}）。\par
於此章中，將會以兩種方法預測使用者個人資訊與大六性格特質分數，分別為基於監督式學習之預測，以及透過將使用者分群後再以監督式學習進行預測，並分別介紹其中所使用之分類器以及分群方法。

}


\section{預測個人資訊之分類模型選擇}
{
由於預測個人資訊屬於分類問題，因此選用 k-Nearest Neighbor（k-NN）~\cite{altman1992introduction}、\\
Random forests ~\cite{ho1995random}、Logistic regression 以及 Support vector machine（SVM） 作為分類器進行個人資訊之預測。其中 k-NN 主要透過參考以使用者之特徵當作座標，找出當前使用者與其他使用者之中最接近之 k 位使用者當作預測依據。Random forests 則是以使用者之特徵當作分支條件透過 decision tree 的方式找出使用者之種類。Logistic regression 則是透過給予所有使用者特徵權重相加，與 Linear regression 方法類似，差別在於 Logistic regression 透過激勵函數（activation function）將輸出值壓縮至 0 到 1 之間成為判斷類別之機率。SVM 為 Hinge loss（式 4.1，$y$ 為預測值、$t$ 為實際值）加上透過 Kernel method 解決特徵向量投影到高維度平面後計算負擔過大的問題之方法。 

$$L(y)=\max (0, 1-t\cdot y) \eqno {(4.1)}$$

}
\section{預測大六性格特質分數之回歸模型選擇}
{
由於大六性格特質分數屬於數值回歸問題，我嘗試了 Linear regression 以及 SVM。其中基於 Linear regression 我將介紹 Lasso regression、 Ridge regression 以及 Elastic net regression，並在第五章進行不同模型之預測結果比較。首先，Lasso regression ~\cite{tibshirani1996regression}為線性最小平方法（linear least squares function）作為 loss function，並經過 L1-norm regularization 之方法，其目標函數定義為下列式 4.2，其主要優勢在於以 L1-norm regularization 所找到最佳解之變數係數經常為 0，因此能夠解決變數選擇問題。\par

    $$\beta=\arg \min_\beta \left[ \frac{1}{2}\sum_{i=1}^n(\hat{y_i} - y_i)^2+\lambda \parallel \beta \parallel_1\right]\eqno {(4.2)}$$

Ridge regression ~\cite{hoerl1970ridge}同樣以線性最小平方法（linear least squares function）作為 loss function，但與 Lasso regression 不同之處在於 Ridge regression 是 L2-norm regularization，因此 Ridge regression 之目標函數可定義成下列式 4.3。\par

    $$\beta=\arg \min_\beta \left[ \frac{1}{2}\sum_{i=1}^n(\hat{y_i} - y_i)^2+\lambda \parallel \beta \parallel_2\right]\eqno {(4.3)}$$
 
而 Elastic net regression 則是綜合了 Lasso regression 與 Ridge regression，同時具有  L1-norm 與  L2-norm regularization，因此 Elastic net regression 之目標函數可定義成下列式 4.4。

    $$\beta=\arg \min_\beta \left[ \frac{1}{2}\sum_{i=1}^n(\hat{y_i} - y_i)^2+\lambda_1 \parallel \beta \parallel_1+\lambda_2 \parallel \beta \parallel_2\right]\eqno {(4.3)}$$

\clearpage
}

\section{結合分群方法之監督式學習}
{
基於相似網頁瀏覽習慣之使用者可能具有類似的特徵分布，因此我猜想若將使用者分群後，將類似的使用者以監督式學習進行個人資訊與大六性格分數預測，相對於直接以全部的使用者特徵進行預測是能夠提高預測結果之準確性。\par

將使用者依據其特徵利用 $k$ - 平均演算法（k-means clustering）進行分群，其目標函數定義為式 4.4，其中 $(x_1,x_2,…,x_n)$ 代表 $n$ 位使用者之特徵向量，將 $n$ 位使用者歸類成 $k$ 個類別 $S=(S_1,S_2,…,S_k)$，$\mu_i$ 為 $S_i$ 中所有使用者的特徵向量平均值，並使其盡量滿足每個類別中平方和（within-cluster sum of squares）為最小值。\par

    $$S=\arg \min_S \sum_{i=1}^k \sum_{x\in S_i} \parallel x-\mu_i\parallel^2 \eqno {(4.4)}$$
    
透過 k-means 將使用者分群之後，再藉由 4.1、4.2 所介紹之方法對每一群使用者進行訓練，並預測其個人資訊以及大六性格特質分數。
\clearpage
}